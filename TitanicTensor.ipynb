{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TitanicTensor.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNPwZ7G4HeV6Hs1PBjVpuwA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdeihim/CPE322/blob/master/TitanicTensor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGA4Ettt9x2J",
        "colab_type": "text"
      },
      "source": [
        "# Titanic Linear Estimator Using TensorFlow\n",
        "\n",
        "The goal of this tutorial will be to take in numeric and categorical data about the survivors and passengers who were killed on the Titanic. Using this data, we will train a linear model to make a prediction about the survival of individuals on board the Titanic that the model hasn't seen yet.\n",
        "\n",
        "Our objectives are:\n",
        "1. Load the proper datasets as a pandas dataframe\n",
        "2. Preprocess the data\n",
        "3. Use a tensorflow model to predict the outcome of passenger's survival"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1fCXWO-CpAL",
        "colab_type": "text"
      },
      "source": [
        "# Setup\n",
        "This is particularly easy in Google Collaboratory, because the necessary libraries are readily available and will not have to be downloaded onto your local machine. You can go ahead and run this block of code below to get these on your current notebook:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2EMTXinAN3r",
        "colab_type": "code",
        "outputId": "d83937d2-b4ee-4cfe-f19a-fbf4671e4b93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!pip install -q sklearn\n",
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from six.moves import urllib\n",
        "import tensorflow.compat.v2.feature_column as fc\n",
        "\n",
        "import tensorflow as tf  # now import the tensorflow module"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x  # this line is not required unless you are in a notebook`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5HEEe_QDDpS",
        "colab_type": "text"
      },
      "source": [
        "# Loading the Data\n",
        "In this section, we will be using the pandas library to read data from a csv file and save as a dataframe. The data is already split into two different datasets, one for training the model, and one for evaluation. For more information on pandas.read_csv see the documentation here: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RhVUfNeHIg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loads datasets as a dataframe using pandas, read_csv\n",
        "dftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv') # training data\n",
        "dfeval = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv') # testing data\n",
        "\n",
        "#This shows the first five entries of the training data.\n",
        "dftrain.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2LSTfchHpqD",
        "colab_type": "text"
      },
      "source": [
        "# Pre-Processing the Data\n",
        "From this dataframe, we can see that the feature columns are the sex, siblings, parch, class, deck, age, and fare. The label, or the column that we are attempting to predict is the \"survived\" column. In order to train against this column, we must remove it from the training and evaluation set, using \n",
        "pop()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmtyPGUrI3IO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#removes survived column and save to new variable\n",
        "y_train = dftrain.pop('survived') #variable for the the survived column at the corresponding index\n",
        "y_eval = dfeval.pop('survived')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFgBYnKsI9Xh",
        "colab_type": "text"
      },
      "source": [
        "The data is not all numeric data, so the columns must be split into different classes, numeric columns and categorical columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9vxNZE4J5c5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "18891a42-f234-4b16-ffad-2c41187bafd3"
      },
      "source": [
        "CATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck',\n",
        "                       'embark_town', 'alone']\n",
        "NUMERIC_COLUMNS = ['age', 'fare']\n",
        "\n",
        "feature_columns = []\n",
        "for feature_name in CATEGORICAL_COLUMNS:\n",
        "  vocabulary = dftrain[feature_name].unique()  # gets a list of all unique values from given feature column\n",
        " feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary))\n",
        "for feature_name in NUMERIC_COLUMNS:\n",
        "  feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))\n",
        "\n",
        "  print(feature_columns)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-3f6fdb4e4cc2>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary))\u001b[0m\n\u001b[0m                                                                                                               ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssBYYjHHM-K2",
        "colab_type": "text"
      },
      "source": [
        "# Input Function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6cg9EeyM7f6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#input function/ usually can copy/ determines number of epochs, batches, and batch size\n",
        "def make_input_fn(data_df, label_df, num_epochs=10, shuffle=False, batch_size=32):\n",
        "  def input_function():  # inner function, this will be returned\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))  # create tf.data.Dataset object with data and its label\n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(1000)  # randomizes the order of data\n",
        "    ds = ds.batch(batch_size).repeat(num_epochs)  # split dataset into batches of 32 and repeat process for number of epochs\n",
        "    return ds  # return a batch of the dataset\n",
        "  return input_function  # return a function object for use"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5SCvL7ANlxA",
        "colab_type": "text"
      },
      "source": [
        "# Training and Creating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNRLlZnr9Wmn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "train_input_fn = make_input_fn(dftrain, y_train)  # here we will call the input_function where a dataset object is returned so we can feed to the model\n",
        "eval_input_fn = make_input_fn(dfeval, y_eval, num_epochs=1, shuffle=False)\n",
        "\n",
        "#create model\n",
        "\n",
        "linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns). #uses tensorflow linear regression estimator\n",
        "linear_est.train(train_input_fn) \n",
        "result = linear_est.evaluate(eval_input_fn)\n",
        "\n",
        "clear_output()\n",
        "print(result['accuracy']) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qdyeLdYNtAj",
        "colab_type": "text"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbCtRFFR9ZCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result= list(linear_est.predict(eval_input_fn))\n",
        "print(dfeval.loc[0])\n",
        "print(y_eval.loc[0])\n",
        "print(result[0]['probabilities'][1]) #print the probablity of survival[1] at index 0 (the first passenger)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}